{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3bRRPwMy4oVAH8Nd3pJd5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShobitKapila/WebScrapping/blob/main/Analysst_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1\n",
        "In this assignment you are required to scrape all products from this URL:\n",
        "https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1\n",
        "\n",
        "Need to scrape atleast 20 pages of product listing pages\n",
        "Items to scrape\n",
        "1. Product URL\n",
        "2. Product Name\n",
        "3. Product Price\n",
        "4. Rating\n",
        "5. Number of reviews"
      ],
      "metadata": {
        "id": "acvgepEly1Ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install requests \n",
        "! pip install bs4\n",
        "! pip install csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukDhR8epzCp5",
        "outputId": "398585d9-6c12-46f0-9001-5e6287a726d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.4.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement csv (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for csv\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_product_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    products = []\n",
        "\n",
        "    # Extract product information from the page\n",
        "    product_list = soup.find_all('div', {'data-component-type': 's-search-result'})\n",
        "\n",
        "    for product in product_list:\n",
        "        data = {}\n",
        "\n",
        "        # Extract the required information\n",
        "        data['Product URL'] = 'https://www.amazon.in' + product.find('a', {'class': 'a-link-normal'})['href']\n",
        "        data['Product Name'] = product.find('span', {'class': 'a-size-medium'}).text.strip()\n",
        "        data['Product Price'] = product.find('span', {'class': 'a-price-whole'}).text.strip()\n",
        "\n",
        "        rating = product.find('span', {'class': 'a-icon-alt'})\n",
        "        if rating:\n",
        "            data['Rating'] = rating.text.strip().split()[0]\n",
        "        else:\n",
        "            data['Rating'] = 'Not available'\n",
        "\n",
        "        reviews = product.find('span', {'class': 'a-size-base'})\n",
        "        if reviews:\n",
        "            data['Number of Reviews'] = reviews.text.strip()\n",
        "        else:\n",
        "            data['Number of Reviews'] = '0'\n",
        "\n",
        "        products.append(data)\n",
        "\n",
        "    return products\n",
        "\n",
        "# Set the number of pages to scrape\n",
        "num_pages = 20\n",
        "\n",
        "# Base URL for product listing\n",
        "base_url = 'https://www.amazon.in/s'\n",
        "\n",
        "# Parameters for the initial page\n",
        "params = {\n",
        "    'k': 'bags',\n",
        "    'crid': '2M096C61O4MLT',\n",
        "    'qid': '1653308124',\n",
        "    'sprefix': 'ba,aps,283',\n",
        "    'ref': 'sr_pg_1'\n",
        "}\n",
        "\n",
        "# Create a CSV file to store the scraped data\n",
        "csv_file = open('products.csv', 'w', newline='')\n",
        "csv_writer = csv.DictWriter(csv_file, fieldnames=['Product URL', 'Product Name', 'Product Price', 'Rating', 'Number of Reviews'])\n",
        "csv_writer.writeheader()\n",
        "\n",
        "# Scrape multiple pages\n",
        "for page in range(1, num_pages + 1):\n",
        "    params['ref'] = f'sr_pg_{page}'\n",
        "    response = requests.get(base_url, params=params)\n",
        "    page_products = scrape_product_page(response.url)\n",
        "    csv_writer.writerows(page_products)\n",
        "\n",
        "csv_file.close()\n"
      ],
      "metadata": {
        "id": "LrUyspJg0PEn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2\n",
        "With the Product URL received in the above case, hit each URL, and add below items:\n",
        "1. Description\n",
        "2. ASIN\n",
        "3. Product Description\n",
        "4. Manufacturer\n",
        "\n",
        "Need to hit around 200 product URLâ€™s and fetch various information.\n",
        "The entire data needs to be exported in a csv format\n"
      ],
      "metadata": {
        "id": "NtudL2Q9oK7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_product_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    data = {}\n",
        "\n",
        "    # Extract the required information\n",
        "    data['Product URL'] = url\n",
        "    data['Product Name'] = soup.find('span', {'id': 'productTitle'}).text.strip()\n",
        "\n",
        "    price = soup.find('span', {'class': 'a-offscreen'})\n",
        "    if price:\n",
        "        data['Product Price'] = price.text.strip()\n",
        "    else:\n",
        "        data['Product Price'] = 'Not available'\n",
        "\n",
        "    rating = soup.find('span', {'class': 'a-icon-alt'})\n",
        "    if rating:\n",
        "        data['Rating'] = rating.text.strip().split()[0]\n",
        "    else:\n",
        "        data['Rating'] = 'Not available'\n",
        "\n",
        "    reviews = soup.find('span', {'id': 'acrCustomerReviewText'})\n",
        "    if reviews:\n",
        "        data['Number of Reviews'] = reviews.text.strip()\n",
        "    else:\n",
        "        data['Number of Reviews'] = '0'\n",
        "\n",
        "    description = soup.find('div', {'id': 'productDescription'})\n",
        "    if description:\n",
        "        data['Description'] = description.text.strip()\n",
        "    else:\n",
        "        data['Description'] = 'Not available'\n",
        "\n",
        "    asin = soup.find('th', text='ASIN')\n",
        "    if asin:\n",
        "        data['ASIN'] = asin.find_next_sibling('td').text.strip()\n",
        "    else:\n",
        "        data['ASIN'] = 'Not available'\n",
        "\n",
        "    product_description = soup.find('div', {'id': 'productDescription'})\n",
        "    if product_description:\n",
        "        data['Product Description'] = product_description.text.strip()\n",
        "    else:\n",
        "        data['Product Description'] = 'Not available'\n",
        "\n",
        "    manufacturer = soup.find('th', text='Manufacturer')\n",
        "    if manufacturer:\n",
        "        data['Manufacturer'] = manufacturer.find_next_sibling('td').text.strip()\n",
        "    else:\n",
        "        data['Manufacturer'] = 'Not available'\n",
        "\n",
        "    return data\n",
        "\n",
        "# Read the product URLs from products.csv\n",
        "product_urls = []\n",
        "with open('products.csv', 'r') as csv_file:\n",
        "    csv_reader = csv.DictReader(csv_file)\n",
        "    for row in csv_reader:\n",
        "        product_urls.append(row['Product URL'])\n",
        "\n",
        "# Create a CSV file to store the scraped data\n",
        "csv_file = open('output.csv', 'w', newline='')\n",
        "csv_writer = csv.DictWriter(csv_file, fieldnames=['Product URL', 'Product Name', 'Product Price', 'Rating', 'Number of Reviews', 'Description', 'ASIN', 'Product Description', 'Manufacturer'])\n",
        "csv_writer.writeheader()\n",
        "\n",
        "# Iterate over the product URLs\n",
        "for url in product_urls:\n",
        "    product_data = scrape_product_page(url)\n",
        "    csv_writer.writerow(product_data)\n",
        "\n",
        "csv_file.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "kimIoPofoWPi",
        "outputId": "20e78140-b000-40c2-8d5e-1359fa629b3f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-a316a3a76446>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# Iterate over the product URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproduct_urls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mproduct_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_product_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mcsv_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-a316a3a76446>\u001b[0m in \u001b[0;36mscrape_product_page\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Extract the required information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Product URL'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Product Name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'productTitle'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'span'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'a-offscreen'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'strip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def scrape_product_page(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    data = {}\n",
        "\n",
        "    # Extract the required information\n",
        "    data['Product URL'] = url\n",
        "    product_title = soup.find('span', {'id': 'productTitle'})\n",
        "    if product_title:\n",
        "        data['Product Name'] = product_title.text.strip()\n",
        "    else:\n",
        "        data['Product Name'] = 'Not available'\n",
        "\n",
        "    price = soup.find('span', {'class': 'a-offscreen'})\n",
        "    if price:\n",
        "        data['Product Price'] = price.text.strip()\n",
        "    else:\n",
        "        data['Product Price'] = 'Not available'\n",
        "\n",
        "    rating = soup.find('span', {'class': 'a-icon-alt'})\n",
        "    if rating:\n",
        "        data['Rating'] = rating.text.strip().split()[0]\n",
        "    else:\n",
        "        data['Rating'] = 'Not available'\n",
        "\n",
        "    reviews = soup.find('span', {'id': 'acrCustomerReviewText'})\n",
        "    if reviews:\n",
        "        data['Number of Reviews'] = reviews.text.strip()\n",
        "    else:\n",
        "        data['Number of Reviews'] = '0'\n",
        "\n",
        "    description = soup.find('div', {'id': 'productDescription'})\n",
        "    if description:\n",
        "        data['Description'] = description.text.strip()\n",
        "    else:\n",
        "        data['Description'] = 'Not available'\n",
        "\n",
        "    asin = soup.find('th', text='ASIN')\n",
        "    if asin:\n",
        "        data['ASIN'] = asin.find_next_sibling('td').text.strip()\n",
        "    else:\n",
        "        data['ASIN'] = 'Not available'\n",
        "\n",
        "    product_description = soup.find('div', {'id': 'productDescription'})\n",
        "    if product_description:\n",
        "        data['Product Description'] = product_description.text.strip()\n",
        "    else:\n",
        "        data['Product Description'] = 'Not available'\n",
        "\n",
        "    manufacturer = soup.find('th', text='Manufacturer')\n",
        "    if manufacturer:\n",
        "        data['Manufacturer'] = manufacturer.find_next_sibling('td').text.strip()\n",
        "    else:\n",
        "        data['Manufacturer'] = 'Not available'\n",
        "\n",
        "    return data\n",
        "\n",
        "# Read the product URLs from products.csv\n",
        "product_urls = []\n",
        "with open('products.csv', 'r') as csv_file:\n",
        "    csv_reader = csv.DictReader(csv_file)\n",
        "    for row in csv_reader:\n",
        "        product_urls.append(row['Product URL'])\n",
        "\n",
        "# Create a CSV file to store the scraped data\n",
        "csv_file = open('output.csv', 'w', newline='')\n",
        "csv_writer = csv.DictWriter(csv_file, fieldnames=['Product URL', 'Product Name', 'Product Price', 'Rating', 'Number of Reviews', 'Description', 'ASIN', 'Product Description', 'Manufacturer'])\n",
        "csv_writer.writeheader()\n",
        "\n",
        "# Iterate over the product URLs\n",
        "for url in product_urls:\n",
        "    product_data = scrape_product_page(url)\n",
        "    csv_writer.writerow(product_data)\n",
        "\n",
        "csv_file.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUawk13bDGmZ",
        "outputId": "8748aaf0-105c-471d-8934-8c763e9ad13b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-45520692ab4e>:43: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  asin = soup.find('th', text='ASIN')\n",
            "<ipython-input-8-45520692ab4e>:55: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
            "  manufacturer = soup.find('th', text='Manufacturer')\n"
          ]
        }
      ]
    }
  ]
}